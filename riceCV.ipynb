{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rice CV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import pathlib\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path=pathlib.Path().absolute()\n",
    "tr_xs_dir= cur_path.parent.joinpath( pathlib.PureWindowsPath( r'data\\Train_Dev\\training') ) #may use match\n",
    "tr_ys_dir= cur_path.parent.joinpath( pathlib.PureWindowsPath( r'data\\Train_Dev\\train_labels') )\n",
    "\n",
    "tr_ys_fls=[ i.stem for i in tr_ys_dir.iterdir() ]\n",
    "dt_sample_num=0 #29 crazy small size #1 large size data #28 normal small, third num boxes, but quite close to 2nd num\n",
    "df_ys= pd.read_csv(  tr_ys_dir.joinpath(tr_ys_fls[dt_sample_num]+'.csv') , names=['x','y']  )\n",
    "#print(df_ys)\n",
    "\n",
    "print('df shape:',df_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img= Image.open( tr_xs_dir.joinpath(tr_ys_fls[dt_sample_num]+'.JPG') )\n",
    "img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image 29 有4007pts，最多可能估5000較保險\n",
    "image 是(1728, 2304, 3), (2000, 3000, 3) 兩種size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是把單純的點出水稻位置問題擴展成RCNN系列的region proposal 問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showBox=True\n",
    "showPoint=False\n",
    "showGrid=False\n",
    "img_sample= mpimg.imread( tr_xs_dir.joinpath(tr_ys_fls[dt_sample_num]+'.JPG') )\n",
    "\n",
    "print('img shape:',img_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pts(cen_pts,sq_width):\n",
    "    anch_pts= cen_pts-sq_width//2\n",
    "    end_pts= cen_pts+sq_width//2\n",
    "    edgs_lengths=end_pts-anch_pts\n",
    "    return anch_pts, end_pts, edgs_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( figsize=[6.4*4, 4.8*4] )\n",
    "\n",
    "sq_width=60 #60 for No0, #26 for No28\n",
    "n_h, n_w,n_c=img_sample.shape\n",
    "print('ori image shape:', img_sample.shape)\n",
    "expd_edge_width=sq_width//2\n",
    "expd_edge_width=0\n",
    "\n",
    "expd_img=np.zeros([n_h+expd_edge_width*2,n_w+expd_edge_width*2,n_c], dtype=np.uint8)\n",
    "expd_img[expd_edge_width:expd_edge_width+n_h, expd_edge_width:expd_edge_width+n_w, :]=img_sample\n",
    "\n",
    "img_sample=expd_img\n",
    "n_h, n_w,n_c=img_sample.shape\n",
    "print('expand image shape:', img_sample.shape)\n",
    "\n",
    "ax.imshow( img_sample )\n",
    "if showPoint:\n",
    "    ax.scatter(df_ys['x']+expd_edge_width,df_ys['y']+expd_edge_width, 2,'r')\n",
    "\n",
    "\n",
    "anch_pts, end_pts, edgs_lengths= get_pts(df_ys.values,sq_width)\n",
    "anch_pts, end_pts= anch_pts+expd_edge_width, end_pts+expd_edge_width\n",
    "\n",
    "# draw bounding box\n",
    "if showBox:\n",
    "    for i in range(df_ys.shape[0]):\n",
    "        ax.add_patch( patches.Rectangle(anch_pts[i],edgs_lengths[i][0],edgs_lengths[i][1],\n",
    "                                    edgecolor = 'blue',\n",
    "                                    fill=False) )\n",
    "\n",
    "    \n",
    "    \n",
    "pic_sp_width=sq_width//2* 7 #multi the grid nuber of decision network\n",
    "#pic_sp_width=#nw//(n_w//pic_sp_width)\n",
    "\n",
    "if showGrid:\n",
    "    plt.hlines([i*pic_sp_width for i in range(n_h//pic_sp_width+1)],0,n_w)\n",
    "    #plt.hlines([i*pic_sp_width for i in range(n_h//pic_sp_width+1)],0,n_w)\n",
    "\n",
    "    plt.vlines([i*pic_sp_width for i in range(n_w//pic_sp_width+1)],0,n_h)\n",
    "    #plt.vlines([i*pic_sp_width for i in range(n_w//pic_sp_width+1)],0,n_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自動pred box width的算法\n",
    "開發自動pred box width的算法，圖中的9個點與其相鄰點距離的平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_nearest_pts(df_ys, pt_sample, n, x_name='x', y_name='y'):\n",
    "    df_ys_find=df_ys.copy()\n",
    "    df_ys_find['l2_square'] = (df_ys[x_name]-pt_sample[0])**2+(df_ys[y_name]-pt_sample[1])**2\n",
    "    pt_n2=df_ys_find.sort_values('l2_square').iloc[1:1+n,:] # n nearest points\n",
    "    pt_n2['distance']=np.sqrt(pt_n2['l2_square'])\n",
    "    pt_n2.drop('l2_square', inplace=True, axis=1)\n",
    "    return pt_n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure( figsize=[6.4*4, 4.8*4] )\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "xv,yv = np.meshgrid( range(n_w//4,n_w,n_w//4), range(n_h//4,n_h,n_h//4) )\n",
    "xv=xv.flatten()\n",
    "yv=yv.flatten()\n",
    "pt_sample=[]\n",
    "for i in range(xv.size):\n",
    "    pt=find_n_nearest_pts(df_ys, [xv[i],yv[i]],1 )[['x','y']]\n",
    "    pt_sample.append(pt)\n",
    "\n",
    "pt_sample = pd.concat(pt_sample)\n",
    "# pt_sample.iloc[0,:]=[ 889,1522 ] #ill case img28\n",
    "print(pt_sample)\n",
    "\n",
    "pts_nearest=[]\n",
    "for i in pt_sample.iterrows():\n",
    "    nearest_pt = find_n_nearest_pts(df_ys, i[1].values, 1)\n",
    "    pts_nearest.append(nearest_pt)\n",
    "pts_nearest=pd.concat(pts_nearest)\n",
    "print( pts_nearest )\n",
    "\n",
    "pred_width=int((np.average( pts_nearest['distance'].values )) *1)+1 #0.8\n",
    "print( 'pred_width:',pred_width )\n",
    "\n",
    "\n",
    "plt.scatter( pt_sample['x']+expd_edge_width, pt_sample['y']+expd_edge_width, 30, 'r')\n",
    "plt.scatter( pts_nearest['x']+expd_edge_width, pts_nearest['y']+expd_edge_width, 30, 'y')\n",
    "plt.imshow(img_sample )\n",
    "\n",
    "anch_pts, end_pts, edgs_lengths= get_pts(df_ys.values,pred_width)\n",
    "anch_pts, end_pts= anch_pts+expd_edge_width, end_pts+expd_edge_width\n",
    "\n",
    "# draw bounding box\n",
    "if showBox:\n",
    "    for i in range(df_ys.shape[0]):\n",
    "        ax.add_patch( patches.Rectangle(anch_pts[i],edgs_lengths[i][0],edgs_lengths[i][1],\n",
    "                                    edgecolor = 'blue',\n",
    "                                    fill=False) )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google object detection api 提供的現成model 最大就是1024x1024\n",
    "這算的上是這個空拍圖辨視的極限，再小就很難框了。對於第28張圖，這甚至是不太ok的\n",
    "\n",
    "# create tf record\n",
    "now, let's create tf record data\n",
    "\n",
    "the code following is base on [ models/research/object_detection/g3doc/using_your_own_dataset.md ](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md), thought the code is not up to date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showBoxAndImage(img, anch_pts, end_pts):\n",
    "    plt.figure(figsize=[6.4*1, 4.8*1])\n",
    "    plt.imshow( img )\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    edgs_lengths = end_pts-anch_pts\n",
    "\n",
    "    # draw bounding box\n",
    "    for i in range(anch_pts.shape[0]):\n",
    "        ax.add_patch( patches.Rectangle(anch_pts[i],edgs_lengths[i][0],edgs_lengths[i][1],\n",
    "                                    edgecolor = 'blue',\n",
    "                                    fill=False) )\n",
    "\n",
    "    plt.show(file_path)\n",
    "def readImgBytes(img_path):\n",
    "    fd = open( img_path, mode='rb')\n",
    "    img_str = fd.read()\n",
    "    fd.close()\n",
    "    return img_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_detection_data( file_path, label_path ):\n",
    "    '''\n",
    "    file_path=img_path, label_path should be pathlib object\n",
    "    '''\n",
    "    df_ys= pd.read_csv( label_path, names=['x','y']  )\n",
    "    filename=file_path.name.encode() #should be bytes\n",
    "    encoded_image_data = readImgBytes(file_path)\n",
    "    img=Image.open(io.BytesIO(encoded_image_data))\n",
    "    width, height = img.size\n",
    "    image_format =file_path.suffix[1:].encode()\n",
    "    \n",
    "    xv,yv = np.meshgrid( range(width//4,width,width//4), range(height//4,height,height//4) )\n",
    "    xv=xv.flatten()\n",
    "    yv=yv.flatten()\n",
    "    pt_sample=[]\n",
    "    for i in range(xv.size):\n",
    "        pt=find_n_nearest_pts(df_ys, [xv[i],yv[i]],1 )[['x','y']]\n",
    "        pt_sample.append(pt)\n",
    "\n",
    "    pt_sample = pd.concat(pt_sample)\n",
    "    #print(pt_sample)\n",
    "\n",
    "    pts_nearest=[]\n",
    "    for i in pt_sample.iterrows():\n",
    "        nearest_pt = find_n_nearest_pts(df_ys, i[1].values, 1)\n",
    "        pts_nearest.append(nearest_pt)\n",
    "    pts_nearest=pd.concat(pts_nearest)\n",
    "    #print( pts_nearest )\n",
    "\n",
    "    pred_width=int((np.average( pts_nearest['distance'].values )) *1)+1 #0.8\n",
    "    #print( 'pred_width:',pred_width )\n",
    "    anch_pts, end_pts, edgs_lengths= get_pts(df_ys.values, pred_width)\n",
    "    \n",
    "    xmins= anch_pts[:,0]/width\n",
    "    xmaxs= end_pts[:,0]/width\n",
    "    ymins= anch_pts[:,1]/height\n",
    "    ymaxs= end_pts[:,1]/height\n",
    "    classes_text= [b'rice']*df_ys.shape[0] #b'' mean encode\n",
    "    classes= [1]*df_ys.shape[0] #class\n",
    "    \n",
    "    return height, width, filename, encoded_image_data, image_format, xmins, xmaxs, ymins, ymaxs, classes_text, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "import io\n",
    "\n",
    "#already deprecate in tf2 --> Module: tf.compat.v1.flags\n",
    "# if realy need flag, use absl-py\n",
    "#flags = tf.app.flags\n",
    "\n",
    "# from absl import app\n",
    "# from absl import flags\n",
    "\n",
    "# flags.DEFINE_string('output_path', str(cur_path.parent.joinpath(r'data')), 'Path to output TFRecord')\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def main(_):\n",
    "#writer = tf.python_io.TFRecordWriter(FLAGS.output_path) #tf.compat.v1.python_io.TFRecordWriter(filename)\n",
    "#https://github.com/tensorflow/models/issues/4794#issuecomment-497315185\n",
    "cur_path=pathlib.Path().absolute()\n",
    "tr_xs_dir= cur_path.parent.joinpath( pathlib.PureWindowsPath( r'data\\Train_Dev\\training') ) #may use match\n",
    "tr_ys_dir= cur_path.parent.joinpath( pathlib.PureWindowsPath( r'data\\Train_Dev\\train_labels') )\n",
    "tr_ys_fls=[ i.stem for i in tr_ys_dir.iterdir() ]\n",
    "writer=tf.io.TFRecordWriter( str(cur_path.parent.joinpath(r'data/riceOdTrain.tfrecord')) ) #input output path\n",
    "# print(writer)\n",
    "\n",
    "# TODO(user): Write code to read in your dataset to examples variable\n",
    "\n",
    "for fl_name in tr_ys_fls:\n",
    "    print('fl_name:',fl_name)\n",
    "    label_path= tr_ys_dir.joinpath( fl_name+'.csv')\n",
    "    file_path= tr_xs_dir.joinpath( fl_name+'.JPG')\n",
    "    height, width, filename, encoded_image_data, image_format, xmins, xmaxs, ymins, ymaxs, classes_text, classes=\\\n",
    "    get_object_detection_data( file_path, label_path )\n",
    "    \n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/height': dataset_util.int64_feature(height),\n",
    "      'image/width': dataset_util.int64_feature(width),\n",
    "      'image/filename': dataset_util.bytes_feature(filename),\n",
    "      'image/source_id': dataset_util.bytes_feature(filename), #source of the original image\n",
    "      'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "      'image/format': dataset_util.bytes_feature(image_format),\n",
    "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "    xmins*=width\n",
    "    xmaxs*=width\n",
    "    ymins*=height\n",
    "    ymaxs*=height\n",
    "    img = Image.open(io.BytesIO(encoded_image_data))\n",
    "    anch_pts= np.concatenate((xmins[:,np.newaxis], ymins[:,np.newaxis]), axis=1)\n",
    "    end_pts= np.concatenate((xmaxs[:,np.newaxis], ymaxs[:,np.newaxis]), axis=1)\n",
    "    #showBoxAndImage(img, anch_pts, end_pts)\n",
    "    \n",
    "    print('============================end=================================\\n')\n",
    "\n",
    "writer.close()\n",
    "print('finish training tfrecord file make')\n",
    "# for example in examples:\n",
    "#     tf_example = create_tf_example(example)\n",
    "#     writer.write(tf_example.SerializeToString())\n",
    "\n",
    "# writer.close()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     tf.app.run()\n",
    "#     app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path=pathlib.Path().absolute()\n",
    "tr_xs_dir= cur_path.parent.joinpath( pathlib.PureWindowsPath( r'data\\sample_data\\ori_image') ) #may use match\n",
    "tr_ys_dir= cur_path.parent.joinpath( pathlib.PureWindowsPath( r'data\\sample_data\\csv_data') )\n",
    "tr_ys_fls=[ i.stem for i in tr_ys_dir.iterdir() ]\n",
    "writer=tf.io.TFRecordWriter( str(cur_path.parent.joinpath(r'data/riceOdEval.tfrecord')) ) #input output path\n",
    "# print(writer)\n",
    "\n",
    "# TODO(user): Write code to read in your dataset to examples variable\n",
    "\n",
    "for fl_name in tr_ys_fls:\n",
    "    print('fl_name:',fl_name)\n",
    "    label_path= tr_ys_dir.joinpath( fl_name+'.csv')\n",
    "    file_path= tr_xs_dir.joinpath( fl_name+'_new.jpg')\n",
    "    height, width, filename, encoded_image_data, image_format, xmins, xmaxs, ymins, ymaxs, classes_text, classes=\\\n",
    "    get_object_detection_data( file_path, label_path )\n",
    "    \n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/height': dataset_util.int64_feature(height),\n",
    "      'image/width': dataset_util.int64_feature(width),\n",
    "      'image/filename': dataset_util.bytes_feature(filename),\n",
    "      'image/source_id': dataset_util.bytes_feature(filename),\n",
    "      'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "      'image/format': dataset_util.bytes_feature(image_format),\n",
    "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "    xmins*=width\n",
    "    xmaxs*=width\n",
    "    ymins*=height\n",
    "    ymaxs*=height\n",
    "    img = Image.open(io.BytesIO(encoded_image_data))\n",
    "    anch_pts= np.concatenate((xmins[:,np.newaxis], ymins[:,np.newaxis]), axis=1)\n",
    "    end_pts= np.concatenate((xmaxs[:,np.newaxis], ymaxs[:,np.newaxis]), axis=1)\n",
    "    #showBoxAndImage(img, anch_pts, end_pts)\n",
    "    \n",
    "    print('============================end=================================\\n')\n",
    "\n",
    "writer.close()\n",
    "print('finish eval tfrecord file make')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## development dataset\n",
    "google object detection api 沒有doc (g3doc)說 inference 的tfrecord 要給什麼東西，google之後也沒有，我只能盡量猜一下\n",
    "\n",
    "更正，inference 就是直接輸照片進去就ok 了，不用任何的 tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_eval_data( file_path ):\n",
    "#     '''\n",
    "#     file_path=img_path, label_path should be pathlib object\n",
    "#     '''\n",
    "#     filename=file_path.name.encode() #should be bytes\n",
    "#     encoded_image_data = readImgBytes(file_path)\n",
    "#     img=Image.open(io.BytesIO(encoded_image_data))\n",
    "#     width, height = img.size\n",
    "#     image_format =file_path.suffix[1:].encode()\n",
    "    \n",
    "#     return height, width, filename, encoded_image_data, image_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_path=pathlib.Path().absolute()\n",
    "# tr_xs_dir= cur_path.parent.joinpath( pathlib.PureWindowsPath( r'data\\Train_Dev\\development') ) #may use match\n",
    "# tr_xs_fls=[ i.stem for i in tr_xs_dir.iterdir() ]\n",
    "# writer= tf.io.TFRecordWriter( str(cur_path.parent.joinpath(r'data/riceOdDvlp.tfrecord')) ) #output path of input file\n",
    "# # print(writer)\n",
    "\n",
    "# # TODO(user): Write code to read in your dataset to examples variable\n",
    "\n",
    "# for fl_name in tr_xs_fls:\n",
    "#     print('fl_name:',fl_name)\n",
    "#     file_path= tr_xs_dir.joinpath( fl_name+'.JPG')\n",
    "#     height, width, filename, encoded_image_data, image_format= get_eval_data( file_path )\n",
    "    \n",
    "#     tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "#       'image/height': dataset_util.int64_feature(height),\n",
    "#       'image/width': dataset_util.int64_feature(width),\n",
    "#       'image/filename': dataset_util.bytes_feature(filename),\n",
    "#       'image/source_id': dataset_util.bytes_feature(filename),\n",
    "#       'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "#       'image/format': dataset_util.bytes_feature(image_format),\n",
    "#     }))\n",
    "#     writer.write(tf_example.SerializeToString())\n",
    "#     print('============================end=================================\\n')\n",
    "\n",
    "# writer.close()\n",
    "# print('finish evaluation tfrecord file make')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
