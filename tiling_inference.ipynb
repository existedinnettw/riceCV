{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419508e7",
   "metadata": {},
   "source": [
    "# tiling inference\n",
    "\n",
    "回歸正常的流程，就是拿到大圖才tiling，tiling inference each image 再combine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(util)\n",
    "\n",
    "# read in tf train example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from util import cur_path, to_abs_path, get_jpg_paths_from_dir, to_aidea_save, zip_dir, get_edge_tile_px, split_img\n",
    "import util \n",
    "import zipfile\n",
    "import cv2\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util, np_box_list, np_box_list_ops, dataset_util\n",
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ece974",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR_LIST=[util.to_abs_path( pathlib.PurePath( 'data/test_public'), util.cur_path.parent),\n",
    "               util.to_abs_path( pathlib.PurePath( 'data/test_private'), util.cur_path.parent),\n",
    "               ]\n",
    "\n",
    "IMAGE_PATHS_LIST=[ util.get_jpg_paths_from_dir(i) for i in IMAGE_DIR_LIST ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH_TO_MODEL_DIR = '/mnt/c/Users/insleker/Google Drive/workspace/riceCV/code/exported_models/centernet_resnet50_v1_fpn_512x512_coco17_tpu-8'\n",
    "PATH_TO_MODEL_DIR = '/mnt/c/Users/insleker/Google Drive/workspace/riceCV/code/exported_models/tile_centernet_resnet50_v1_fpn_512x512_coco17_tpu-8'\n",
    "#PATH_TO_MODEL_DIR = '/mnt/c/Users/insleker/Google Drive/workspace/riceCV/code/exported_models/centernet_hg104_512x512_coco17_tpu-8'\n",
    "\n",
    "model_p= pathlib.PurePath(PATH_TO_MODEL_DIR)\n",
    "MODEL_NAME= model_p.parts[-1]\n",
    "\n",
    "LABEL_FILENAME = 'rice_label_map.pbtxt'\n",
    "PATH_TO_LABELS = '/mnt/c/Users/insleker/Google Drive/workspace/riceCV/data/rice_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# Load saved model and build the detection function\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4619d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "modify ver, which iou become only area of smaller box as denominator\n",
    "after calculate the output num, it seem have bug\n",
    "'''\n",
    "def nms(rects, thd=0.5):\n",
    "    \"\"\"\n",
    "    Filter rectangles\n",
    "    rects is array of oblects ([x1,y1,x2,y2], confidence, class)\n",
    "    thd - intersection threshold (intersection divides min square of rectange)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    remove = [False] * len(rects)\n",
    "\n",
    "    for i in range(0, len(rects) - 1):\n",
    "        if remove[i]:\n",
    "            continue\n",
    "        inter = [0.0] * len(rects)\n",
    "        for j in range(i, len(rects)):\n",
    "            if remove[j]:\n",
    "                continue\n",
    "            inter[j] = intersection(rects[i][0], rects[j][0]) / min(square(rects[i][0]), square(rects[j][0]))\n",
    "\n",
    "        max_prob = 0.0\n",
    "        max_idx = 0\n",
    "        for k in range(i, len(rects)):\n",
    "            if inter[k] >= thd:\n",
    "                if rects[k][1] > max_prob:\n",
    "                    max_prob = rects[k][1]\n",
    "                    max_idx = k\n",
    "\n",
    "        for k in range(i, len(rects)):\n",
    "            if (inter[k] >= thd) & (k != max_idx):\n",
    "                remove[k] = True\n",
    "\n",
    "    for k in range(0, len(rects)):\n",
    "        if not remove[k]:\n",
    "            out.append(rects[k])\n",
    "\n",
    "    boxes = [box[0] for box in out]\n",
    "    scores = [score[1] for score in out]\n",
    "    classes = [cls[2] for cls in out]\n",
    "    return boxes, scores, classes\n",
    "\n",
    "\n",
    "def intersection(rect1, rect2):\n",
    "    \"\"\"\n",
    "    Calculates square of intersection of two rectangles\n",
    "    rect: list with coords of top-right and left-boom corners [x1,y1,x2,y2]\n",
    "    return: square of intersection\n",
    "    \"\"\"\n",
    "    x_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]));\n",
    "    y_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]));\n",
    "    overlapArea = x_overlap * y_overlap;\n",
    "    return overlapArea\n",
    "\n",
    "\n",
    "def square(rect):\n",
    "    \"\"\"\n",
    "    Calculates square of rectangle\n",
    "    \"\"\"\n",
    "    return abs(rect[2] - rect[0]) * abs(rect[3] - rect[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ad6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([1,3])\n",
    "np.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils.np_box_ops import area, intersection, iou, ioa\n",
    "from object_detection.utils.np_box_list_ops import filter_scores_greater_than, sort_by_field, gather\n",
    "# base on original non max suppression, but change the iou to ioa with larger value\n",
    "\n",
    "def modify_non_max_suppression(boxlist,\n",
    "                               max_output_size=10000,\n",
    "                               ioa_threshold=1.0,\n",
    "                               score_threshold=-10.0):\n",
    "  \"\"\"Non maximum suppression.\n",
    "\n",
    "  This op greedily selects a subset of detection bounding boxes, pruning\n",
    "  away boxes that have high IOU (intersection over union) overlap (> thresh)\n",
    "  with already selected boxes. In each iteration, the detected bounding box with\n",
    "  highest score in the available pool is selected.\n",
    "\n",
    "  Args:\n",
    "    boxlist: BoxList holding N boxes.  Must contain a 'scores' field\n",
    "      representing detection scores. All scores belong to the same class.\n",
    "    max_output_size: maximum number of retained boxes\n",
    "    iou_threshold: intersection over union threshold.\n",
    "    score_threshold: minimum score threshold. Remove the boxes with scores\n",
    "                     less than this value. Default value is set to -10. A very\n",
    "                     low threshold to pass pretty much all the boxes, unless\n",
    "                     the user sets a different score threshold.\n",
    "\n",
    "  Returns:\n",
    "    a BoxList holding M boxes where M <= max_output_size\n",
    "  Raises:\n",
    "    ValueError: if 'scores' field does not exist\n",
    "    ValueError: if threshold is not in [0, 1]\n",
    "    ValueError: if max_output_size < 0\n",
    "  \"\"\"\n",
    "  if not boxlist.has_field('scores'):\n",
    "    raise ValueError('Field scores does not exist')\n",
    "  if ioa_threshold < 0. or ioa_threshold > 1.0:\n",
    "    raise ValueError('IOA threshold must be in [0, 1]')\n",
    "  if max_output_size < 0:\n",
    "    raise ValueError('max_output_size must be bigger than 0.')\n",
    "\n",
    "  boxlist = filter_scores_greater_than(boxlist, score_threshold)\n",
    "  if boxlist.num_boxes() == 0:\n",
    "    return boxlist\n",
    "\n",
    "  boxlist = sort_by_field(boxlist, 'scores')\n",
    "\n",
    "  # Prevent further computation if NMS is disabled.\n",
    "  if ioa_threshold == 1.0:\n",
    "    if boxlist.num_boxes() > max_output_size:\n",
    "      selected_indices = np.arange(max_output_size)\n",
    "      return gather(boxlist, selected_indices)\n",
    "    else:\n",
    "      return boxlist\n",
    "\n",
    "  boxes = boxlist.get()\n",
    "  num_boxes = boxlist.num_boxes()\n",
    "  # is_index_valid is True only for all remaining valid boxes,\n",
    "  is_index_valid = np.full(num_boxes, 1, dtype=bool)\n",
    "  selected_indices = []\n",
    "  num_output = 0\n",
    "  for i in range(num_boxes):\n",
    "    if num_output < max_output_size:\n",
    "      if is_index_valid[i]:\n",
    "        num_output += 1\n",
    "        selected_indices.append(i)\n",
    "        is_index_valid[i] = False\n",
    "        valid_indices = np.where(is_index_valid)[0]\n",
    "        if valid_indices.size == 0:\n",
    "          break\n",
    "\n",
    "        intersect_over_union =  ioa(np.expand_dims(boxes[i, :], axis=0), boxes[valid_indices, :])\n",
    "        intersect_over_union = np.squeeze(intersect_over_union, axis=0)\n",
    "        is_index_valid[valid_indices] = np.logical_and(\n",
    "            is_index_valid[valid_indices],\n",
    "            intersect_over_union <= ioa_threshold)\n",
    "        \n",
    "  return gather(boxlist, np.array(selected_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f68d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ba0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRESH= 0.25 #0.35\n",
    "T_ROW=5\n",
    "T_COL=8\n",
    "OVLP_PCT=1/4\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "for IMAGE_PATHS in IMAGE_PATHS_LIST:\n",
    "    for image_path in IMAGE_PATHS[:]: #can control image number\n",
    "\n",
    "        print('Running inference for {}... '.format(image_path))\n",
    "\n",
    "        image_np = load_image_into_numpy_array(image_path)\n",
    "        \n",
    "        #do tiling\n",
    "        boxlist=np_box_list.BoxList( np.array([], dtype=np.float).reshape(0,4) ) #[y_min, x_min, y_max, x_max]\n",
    "        boxlist.add_field('scores', np.array([], dtype=np.float) )\n",
    "        sub_img_list, _= split_img(image_np,None,T_ROW,T_COL,OVLP_PCT)\n",
    "        for i in range(T_ROW):\n",
    "            for j in range(T_COL):\n",
    "                sub_img=sub_img_list[i][j]\n",
    "                \n",
    "                # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "                input_tensor = tf.convert_to_tensor(sub_img)\n",
    "                # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "                input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "                # input_tensor = np.expand_dims(image_np, 0)\n",
    "                detections = detect_fn(input_tensor)\n",
    "\n",
    "                # All outputs are batches tensors.\n",
    "                # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "                # We're only interested in the first num_detections.\n",
    "                num_detections = int(detections.pop('num_detections'))\n",
    "                detections = {key: value[0, :num_detections].numpy()\n",
    "                               for key, value in detections.items()}\n",
    "                detections['num_detections'] = num_detections\n",
    "\n",
    "                #viz to check predict\n",
    "                #detection_classes should be ints.\n",
    "#                 detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "#                 image_np_with_detections = sub_img.copy()\n",
    "#                 viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "#                       image_np_with_detections,\n",
    "#                       detections['detection_boxes'],\n",
    "#                       detections['detection_classes'],\n",
    "#                       detections['detection_scores'],\n",
    "#                       category_index,\n",
    "#                       use_normalized_coordinates=True,\n",
    "#                       max_boxes_to_draw=300,\n",
    "#                       min_score_thresh=TRESH)\n",
    "#                 plt.figure( figsize=(6, 6) )\n",
    "#                 plt.imshow(image_np_with_detections)\n",
    "#                 plt.show()\n",
    "#                 print('Done')\n",
    "                \n",
    "                #reverse boxes to ori coordinate\n",
    "                h,w,c= image_np.shape\n",
    "                hh,ww,cc= sub_img.shape\n",
    "                WITH_OVLP_N_ROW, N_ROW= get_edge_tile_px(S=h, T=T_ROW, OVLP_PCT=1/4)\n",
    "                WITH_OVLP_N_COL, N_COL= get_edge_tile_px(S=w, T=T_COL, OVLP_PCT=1/4)\n",
    "                b_y_min= WITH_OVLP_N_ROW*i\n",
    "                b_y_max= b_y_min+N_ROW\n",
    "                b_x_min= WITH_OVLP_N_COL*j\n",
    "                b_x_max= b_x_min+N_COL\n",
    "                window= [-b_y_min/hh, -b_x_min/ww, (h-b_y_min)/hh, (w-b_x_min)/ww] #normalized\n",
    "                \n",
    "                framed_boxes= np_box_list.BoxList(detections['detection_boxes'])\n",
    "                framed_boxes= np_box_list_ops.change_coordinate_frame( framed_boxes, window )\n",
    "                boxlist.data['boxes']= np.concatenate( (boxlist.get(), framed_boxes.get()) )\n",
    "                boxlist.data['scores']= np.concatenate( (boxlist.get_field('scores'), detections['detection_scores']) )\n",
    "                \n",
    "        #non max supression all the boxes in boxlist\n",
    "        print('boxes num before nms:', boxlist.num_boxes() )\n",
    "        \n",
    "#         boxlist= np_box_list_ops.non_max_suppression(boxlist, \n",
    "#                                                      max_output_size=4500,\n",
    "#                                                      iou_threshold=0.2, \n",
    "#                                                      score_threshold=TRESH)\n",
    "        boxlist= modify_non_max_suppression(boxlist, \n",
    "                                             max_output_size=4500,\n",
    "                                             ioa_threshold=0.4, \n",
    "                                             score_threshold=TRESH)\n",
    "\n",
    "#         out_box_list=[]\n",
    "#         for i in range(boxlist.num_boxes()):\n",
    "#             window=boxlist.get()[i]\n",
    "#             out_box_list.append( [[window[1],window[0],window[3],window[2]], boxlist.get_field('scores')[i], 1] )\n",
    "#         boxes, scores, classes= nms(out_box_list, thd=0.3)\n",
    "#         print()\n",
    "#         boxes=np.array(boxes)\n",
    "#         print(boxes.shape)\n",
    "#         boxes= np.stack( [boxes[:,1],boxes[:,0],boxes[:,3],boxes[:,2] ], axis=1 )\n",
    "#         print(boxes.shape)\n",
    "#         boxlist=np_box_list.BoxList( boxes ) #[y_min, x_min, y_max, x_max]\n",
    "#         boxlist.add_field('scores', np.array(scores) )\n",
    "        \n",
    "        print('boxes num aft nms:', boxlist.num_boxes() )\n",
    "\n",
    "        \n",
    "            \n",
    "        #--- save to aidea data, which use score and tresh to filter boxes\n",
    "        print('detect size before thresh:', detections['detection_boxes'].shape)\n",
    "        img_p= pathlib.PurePath(image_path)\n",
    "        img_name= img_p.stem\n",
    "        target_path= img_p.parents[0].joinpath('inference/tli_'+MODEL_NAME+'/'+img_name+'.csv')\n",
    "        print(target_path)\n",
    "        pathlib.Path(target_path.parent).mkdir(parents=True, exist_ok=True) #create folder if not exists\n",
    "        detection_boxes, detection_centers= to_aidea_save(image_np, boxlist.get(), \\\n",
    "                                                          boxlist.get_field('scores'), target_path, thresh=TRESH)\n",
    "\n",
    "        #--- visualize the result\n",
    "#         image_np_with_detections = image_np.copy()\n",
    "#         viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "#                       image_np_with_detections,\n",
    "#                       boxlist.get(),\n",
    "#                       [1]*boxlist.num_boxes(),\n",
    "#                       boxlist.get_field('scores'),\n",
    "#                       category_index,\n",
    "#                       use_normalized_coordinates=True,\n",
    "#                       max_boxes_to_draw=4500,\n",
    "#                       min_score_thresh=TRESH)\n",
    "\n",
    "#         plt.figure( figsize=[6.4*4, 4.8*4] )\n",
    "#         plt.imshow(image_np_with_detections)\n",
    "#         plt.show()\n",
    "        print()\n",
    "    \n",
    "\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path= cur_path.joinpath( 'AIdea_inference_tl_{}.zip'.format(MODEL_NAME) )\n",
    "with zipfile.ZipFile( zip_path, mode='w' ) as zf:\n",
    "    zf.close()\n",
    "for IMAGE_DIR in IMAGE_DIR_LIST:\n",
    "    csvs_dir=IMAGE_DIR.joinpath('inference/tli_'+MODEL_NAME)\n",
    "    print(csvs_dir)\n",
    "    zip_dir( zip_path, csvs_dir, '**/*.csv')\n",
    "    \n",
    "print('finsih zip file, upload plz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c3f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
